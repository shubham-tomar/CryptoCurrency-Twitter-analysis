import re
import tweepy
from tweepy import OAuthHandler
import datetime
import pymongo
from textblob import TextBlob
import string
import pandas as pd
import nltk
from nltk.stem.porter import *
from wordcloud import WordCloud, STOPWORDS
import matplotlib.pyplot as plt


class TwitterClient(object):

    def __init__(self):

        consumer_key = '**********'
        consumer_secret = '********'
        access_token = '*********'
        access_token_secret = '*************'

        connection = pymongo.MongoClient()
        db1 = connection['GlobalTweets']
        db2 = connection['SpecificTweets']
        db1collection = db1["Tweets_from_to"]
        db2collection = db2["Tweets from_to_"]

        try:
            self.auth = OAuthHandler(consumer_key, consumer_secret)
            self.auth.set_access_token(access_token, access_token_secret)
            self.api = tweepy.API(self.auth,wait_on_rate_limit=True)

        except:
            print("Error: Authentication Failed")

        self.getGeneralTweets(db1collection)
        self.getSpecificTweets(db2collection)
        # self.getDatafromDB(db1)

        connection.close()

    def getSpecificTweets(self,db2collection):

        cols = ['user_id','user_name','followers_count','retweet_count','favorite_count', "location",
                'tweet_source','id_created_at','tweet_created_at','tweet_time','OrignalTweet','CleanTweet','polarity']
        profiles = ['VitalikButerin','satoshilite','novogratz','CremeDeLaCrypto','RonnieMoas','lopp',
                    'balajis','onemanatatime','westcoastbill','starkness','NickSzabo4','adam3us','cburniske',
                    'AriannaSimpson','TuurDemeester']
        mainDF=pd.DataFrame(columns = cols)
        # startDate = "2018-12-31"
        for i in profiles:
            # startDate = self.addDaysToDate(startDate, 1)
            # endDate = self.addDaysToDate(startDate, 1)
            # fetched_tweets = self.api.user_timeline(id=i,count=200)
            print(i)
            try:
                for pages in tweepy.Cursor(self.api.user_timeline, id=i,lang="en",since="2019-01-01",
                                           until="2019-01-31",count=200).pages():
                    tweet_data = self.getTweetData(pages)
                    print(tweet_data)
                    tempDF = pd.DataFrame(data=tweet_data,columns=cols)
                    mainDF = pd.concat([mainDF,tempDF],axis=0,ignore_index=True)
            except: continue
        print(mainDF)
        mainDF.to_csv('ProfileSpecificTweets.csv', index=False)
        # data_to_insert = mainDF.to_dict("records")
        # db2collection.insert_many(data_to_insert)

    def getGeneralTweets(self,db1collection):
        cols = ['user_id','user_name','followers_count','retweet_count','favorite_count', "location",
                'tweet_source','id_created_at','tweet_created_at','tweet_time','OrignalTweet','CleanTweet','polarity']
        mainDF = pd.DataFrame(columns=cols)
        query = ['Crypto Currency','trade']
        # startDate = "2019-04-21"
        #startDate = ("2018-12-31" + datetime.timedelta(days=1)).strftime("%Y-%m-%d")
        # startDate = self.addDaysToDate(startDate,1)

        # startDate = (datetime.datetime.today() - datetime.timedelta(days=7)).strftime("%Y-%m-%d")
        # endDate = self.addDaysToDate(startDate,1)
        # print('*****************startDate-endDate',startDate,endDate)
        fetched_tweets = tweepy.Cursor(self.api.search,q=query,lang="en",since="2020-02-11",until="2020-02-19",
                                       include_rts=False).items()
        tweet_data = self.getTweetData(fetched_tweets)
        # print(tweet_data)
        tempDF = pd.DataFrame(data=tweet_data,columns=cols)
        mainDF = pd.concat([mainDF,tempDF],axis=0,ignore_index=True)
        # t = mainDF['CleanTweet']
        # print(mainDF)
        # self.generateCloudWords(t)
        mainDF.to_csv('GeneralTweets.csv', index=False)
        # data_to_insert = mainDF.to_dict("records")
        # db1collection.insert_many(data_to_insert)

    def getTweetData(self,fetched_tweets):
        data = []
        stemmer = PorterStemmer()
        for tweet in fetched_tweets:
            # cleantTweet contains tweet without any hyperlink
            orignalTweet = tweet.text

            cleanTweet = orignalTweet.replace("@", "")
            cleanTweet = self.remove_stopwords(cleanTweet.lower())
            cleanTweet = ''.join(re.sub("([^0-9A-Za-z \t])|(\w+:\/\/\S+)", "", cleanTweet))
            cleanTweet = [stemmer.stem(i) for i in cleanTweet]
            cleanTweet = ''.join([w for w in cleanTweet])
            cleanTweet = ' '.join([w for w in cleanTweet.split() if len(w) > 3])
            pol = TextBlob(cleanTweet)

            data.append([tweet.user.screen_name,tweet.user.name,tweet.user.followers_count,tweet.retweet_count,
                         tweet.favorite_count,tweet.user.location,tweet.source,(tweet.user.created_at).strftime("%Y-%m-%d"),
                         (tweet.created_at).strftime("%Y-%m-%d"),(tweet.created_at).strftime("%H:%M:%S"),orignalTweet,
                         cleanTweet,pol.sentiment.polarity])

        return data

    def remove_stopwords(self,text):
        stopwords = nltk.corpus.stopwords.words('english')
        clean_text = ' '.join([word for word in text.split() if word not in stopwords])
        return clean_text

    # def addDaysToDate(self,start_date,day):
    #     date_1 = datetime.datetime.strptime(start_date, "%Y-%m-%d")
    #     end_date = (date_1 + datetime.timedelta(days=day)).strftime("%Y-%m-%d")
    #     return end_date

    def generateCloudWords(self,t):
        stopwords = set(STOPWORDS)
        w =''
        notin = ['bitcoin','follow','rt','comments','crypto','cryptocurrency','currency','crypto currency',
                 'retweet','trade']
        for tw in t:
            token = tw.split()
            for i in range(len(token)):
                token[i] = token[i].lower()
            for words in token:
                if words not in notin:
                    w = w + words + ' '

        # print(w)
        wordcloud = WordCloud(width=700, height=700,
                              background_color='white',
                              stopwords=stopwords,
                              min_font_size=10).generate(w)
        # plot the WordCloud image

        print('***************')
        plt.figure(figsize=(8,8), facecolor=None)
        plt.imshow(wordcloud)
        plt.axis("off")
        plt.tight_layout(pad=0)
        plt.show()

    # def getDatafromDB(self,dbData):
    #     data = dbData['Tweets_from_to']
    #     t= []
    #     for item in data.find({},{ "user_id": 1, "tweet": 12 }):
    #         t.append(item['tweet'])
    #     self.generateCloudWords(t)

obj = TwitterClient()
